---
phase: 01-task-orchestration
plan: 02
type: tdd
domain: business-logic
---

<objective>
Impl√©menter TaskOrchestrator avec retry intelligent automatique et tracking granulaire, en TDD.

Purpose: Service qui cr√©e et ex√©cute les tasks granulaires, avec retry automatique (skip COMPLETED)
Output: TaskOrchestrator test√© (>90% coverage) + retry automatique fonctionnel
</objective>

<execution_context>
./.claude/get-shit-done/workflows/execute-phase.md
./.claude/get-shit-done/references/tdd.md
./01-02-SUMMARY.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@backend/models/user/marketplace_task.py (cr√©√© en Plan 01-01)
@backend/models/user/marketplace_job.py
@backend/services/marketplace/marketplace_job_processor.py
</context>

<feature>
**TaskOrchestrator** - Orchestrateur de tasks granulaires avec retry intelligent

**R√®gles m√©tier:**
1. **Cr√©ation**: G√©n√®re N tasks ordonn√©es pour un job (ex: validate, upload_image_1, upload_image_2, create_listing)
2. **Ex√©cution**: Ex√©cute tasks s√©quentiellement, **1 commit DB apr√®s chaque task**
3. **Retry intelligent**: Au retry, **skip automatiquement les tasks COMPLETED**, retry seulement les FAILED
4. **Status tracking**: Job reste RUNNING tant que toutes les tasks ne sont pas COMPLETED

**Exemple Flow:**
```
Job "Publier produit X sur Vinted":
  Task 1 (Validate): PENDING ‚Üí RUNNING ‚Üí COMPLETED ‚úÖ ‚Üí commit
  Task 2 (Upload image 1/3): PENDING ‚Üí RUNNING ‚Üí COMPLETED ‚úÖ ‚Üí commit
  Task 3 (Upload image 2/3): PENDING ‚Üí RUNNING ‚Üí FAILED ‚ùå ‚Üí commit
  ‚Üí Job devient FAILED

Retry automatique apr√®s 30s:
  Task 1: status=COMPLETED ‚Üí Skip ‚è©
  Task 2: status=COMPLETED ‚Üí Skip ‚è©
  Task 3: status=FAILED ‚Üí Retry ‚Üí RUNNING ‚Üí COMPLETED ‚úÖ
  Task 4 (Upload image 3/3): PENDING ‚Üí RUNNING ‚Üí COMPLETED ‚úÖ
  Task 5 (Create listing): PENDING ‚Üí RUNNING ‚Üí COMPLETED ‚úÖ
  ‚Üí Job devient COMPLETED
```

**API Publique:**
```python
class TaskOrchestrator:
    def create_tasks(self, job: MarketplaceJob, task_names: List[str]) -> List[MarketplaceTask]
    def execute_task(self, task: MarketplaceTask, handler: Callable) -> TaskResult
    def should_skip_task(self, task: MarketplaceTask) -> bool
    def execute_job_with_tasks(self, job: MarketplaceJob, tasks: List[MarketplaceTask], handlers: Dict) -> bool
```
</feature>

<tasks>

<task type="auto">
  <name>üî¥ RED: Tests cr√©ation de tasks</name>
  <files>backend/tests/unit/services/test_task_orchestrator.py</files>
  <action>
Cr√©er fichier de test pytest avec **tests FIRST** (RED phase):

```python
import pytest
from services.marketplace.task_orchestrator import TaskOrchestrator
from models.user.marketplace_task import MarketplaceTask
from models.user.marketplace_job import MarketplaceJob

def test_create_tasks_generates_ordered_tasks(db_session, sample_job):
    """Should create N tasks with position 1..N."""
    orchestrator = TaskOrchestrator(db_session)

    task_names = ["Validate product", "Upload image 1/2", "Upload image 2/2", "Create listing"]

    tasks = orchestrator.create_tasks(sample_job, task_names)

    assert len(tasks) == 4
    assert tasks[0].name == "Validate product"
    assert tasks[0].position == 1
    assert tasks[0].status == "PENDING"
    assert tasks[1].position == 2
    assert tasks[3].position == 4

def test_create_tasks_links_to_job(db_session, sample_job):
    """All tasks should have job_id = job.id."""
    orchestrator = TaskOrchestrator(db_session)

    tasks = orchestrator.create_tasks(sample_job, ["Task 1", "Task 2"])

    assert all(task.job_id == sample_job.id for task in tasks)

def test_create_tasks_commits_to_db(db_session, sample_job):
    """Tasks should be persisted in DB."""
    orchestrator = TaskOrchestrator(db_session)

    tasks = orchestrator.create_tasks(sample_job, ["Task A"])

    # Query DB to verify persistence
    db_tasks = db_session.query(MarketplaceTask).filter_by(job_id=sample_job.id).all()
    assert len(db_tasks) == 1
    assert db_tasks[0].name == "Task A"
```

**Fixtures n√©cessaires:**
```python
@pytest.fixture
def sample_job(db_session):
    """Create a sample MarketplaceJob for testing."""
    job = MarketplaceJob(
        user_id=1,
        platform="vinted",
        action="publish",
        status="PENDING"
    )
    db_session.add(job)
    db_session.commit()
    return job
```

Tests doivent **FAIL** (pas d'impl√©mentation encore).
  </action>
  <verify>
```bash
cd backend
pytest tests/unit/services/test_task_orchestrator.py::test_create_tasks_generates_ordered_tasks -v
```
Doit FAIL avec "ModuleNotFoundError: No module named 'services.marketplace.task_orchestrator'"
  </verify>
  <done>
- [ ] 3 tests cr√©√©s pour cr√©ation de tasks
- [ ] Fixtures db_session et sample_job d√©finies
- [ ] Tests FAIL comme pr√©vu (RED phase)
  </done>
</task>

<task type="auto">
  <name>üü¢ GREEN: Impl√©mentation cr√©ation tasks</name>
  <files>backend/services/marketplace/task_orchestrator.py</files>
  <action>
Cr√©er TaskOrchestrator avec impl√©mentation **minimale** pour faire passer les tests:

```python
from sqlalchemy.orm import Session
from models.user.marketplace_task import MarketplaceTask
from models.user.marketplace_job import MarketplaceJob
from typing import List, Callable, Dict, Any
from datetime import datetime, timezone

class TaskResult:
    """R√©sultat d'ex√©cution d'une task."""
    def __init__(self, success: bool, result: Dict[str, Any] = None, error: str = None):
        self.success = success
        self.result = result or {}
        self.error = error

class TaskOrchestrator:
    """
    Orchestrateur de tasks granulaires avec retry intelligent.

    Permet de:
    - Cr√©er des tasks ordonn√©es pour un job
    - Ex√©cuter les tasks avec 1 commit/task
    - Retry intelligent: skip les COMPLETED
    """

    def __init__(self, db: Session):
        self.db = db

    def create_tasks(
        self,
        job: MarketplaceJob,
        task_names: List[str]
    ) -> List[MarketplaceTask]:
        """
        Cr√©er des tasks ordonn√©es pour un job.

        Args:
            job: Le MarketplaceJob parent
            task_names: Liste de noms (ex: ["Validate", "Upload image 1/2"])

        Returns:
            Liste des MarketplaceTasks cr√©√©es et persist√©es
        """
        tasks = []

        for position, name in enumerate(task_names, start=1):
            task = MarketplaceTask(
                job_id=job.id,
                name=name,
                position=position,
                status="PENDING"
            )
            self.db.add(task)
            tasks.append(task)

        self.db.commit()
        return tasks
```

**Imports √† ajouter en haut:**
```python
from shared.logging_setup import get_logger
logger = get_logger(__name__)
```

Impl√©mentation simple, juste pour GREEN.
  </action>
  <verify>
```bash
pytest tests/unit/services/test_task_orchestrator.py -k "test_create_tasks" -v
```
Tous les tests de cr√©ation doivent PASS (GREEN phase).
  </verify>
  <done>
- [ ] TaskOrchestrator cr√©√©
- [ ] create_tasks() impl√©ment√©
- [ ] TaskResult helper cr√©√©
- [ ] Tests de cr√©ation PASS (GREEN)
  </done>
</task>

<task type="auto">
  <name>üî¥ RED: Tests ex√©cution tasks</name>
  <files>backend/tests/unit/services/test_task_orchestrator.py</files>
  <action>
Ajouter tests pour l'ex√©cution de tasks (RED phase):

```python
def test_execute_task_transitions_to_running(db_session, sample_task):
    """Task should transition PENDING ‚Üí RUNNING."""
    orchestrator = TaskOrchestrator(db_session)

    def dummy_handler(task):
        return {"success": True}

    result = orchestrator.execute_task(sample_task, dummy_handler)

    # Verify status changed during execution
    db_session.refresh(sample_task)
    assert sample_task.status in ["RUNNING", "COMPLETED"]  # Could be either

def test_execute_task_marks_completed_on_success(db_session, sample_task):
    """Successful execution should mark task as COMPLETED."""
    orchestrator = TaskOrchestrator(db_session)

    def success_handler(task):
        return {"uploaded_id": "vinted_123"}

    result = orchestrator.execute_task(sample_task, success_handler)

    db_session.refresh(sample_task)
    assert sample_task.status == "COMPLETED"
    assert sample_task.result == {"uploaded_id": "vinted_123"}
    assert result.success is True

def test_execute_task_marks_failed_on_exception(db_session, sample_task):
    """Handler exception should mark task as FAILED."""
    orchestrator = TaskOrchestrator(db_session)

    def failing_handler(task):
        raise Exception("Network timeout")

    result = orchestrator.execute_task(sample_task, failing_handler)

    db_session.refresh(sample_task)
    assert sample_task.status == "FAILED"
    assert "Network timeout" in sample_task.error_message
    assert result.success is False

def test_execute_task_sets_timestamps(db_session, sample_task):
    """Should set started_at and completed_at."""
    orchestrator = TaskOrchestrator(db_session)

    result = orchestrator.execute_task(sample_task, lambda t: {})

    db_session.refresh(sample_task)
    assert sample_task.started_at is not None
    assert sample_task.completed_at is not None
```

**Fixture suppl√©mentaire:**
```python
@pytest.fixture
def sample_task(db_session, sample_job):
    """Create a sample task."""
    task = MarketplaceTask(
        job_id=sample_job.id,
        name="Test task",
        position=1,
        status="PENDING"
    )
    db_session.add(task)
    db_session.commit()
    return task
```

Tests doivent FAIL (m√©thode execute_task pas encore impl√©ment√©e).
  </action>
  <verify>
```bash
pytest tests/unit/services/test_task_orchestrator.py::test_execute_task_marks_completed_on_success -v
```
Doit FAIL avec AttributeError (m√©thode manquante).
  </verify>
  <done>
- [ ] 4 tests d'ex√©cution cr√©√©s
- [ ] Fixture sample_task ajout√©e
- [ ] Tests couvrent: status, result, erreurs, timestamps
- [ ] Tests FAIL (RED phase)
  </done>
</task>

<task type="auto">
  <name>üü¢ GREEN: Impl√©mentation ex√©cution</name>
  <files>backend/services/marketplace/task_orchestrator.py</files>
  <action>
Ajouter m√©thode execute_task():

```python
def execute_task(
    self,
    task: MarketplaceTask,
    handler: Callable[[MarketplaceTask], Dict[str, Any]]
) -> TaskResult:
    """
    Ex√©cuter une task avec 1 commit DB.

    Args:
        task: La task √† ex√©cuter
        handler: Fonction qui fait le vrai travail

    Returns:
        TaskResult avec success et result
    """
    try:
        # 1. Marquer RUNNING
        task.status = "RUNNING"
        task.started_at = datetime.now(timezone.utc)
        self.db.commit()

        logger.info(f"Task #{task.id} started: {task.name}")

        # 2. Ex√©cuter handler
        result = handler(task)

        # 3. Marquer COMPLETED
        task.status = "COMPLETED"
        task.completed_at = datetime.now(timezone.utc)
        task.result = result
        self.db.commit()  # 1 commit par task ‚úÖ

        logger.info(f"Task #{task.id} completed: {task.name}")

        return TaskResult(success=True, result=result)

    except Exception as e:
        # Marquer FAILED
        task.status = "FAILED"
        task.error_message = str(e)
        task.completed_at = datetime.now(timezone.utc)
        self.db.commit()

        logger.error(f"Task #{task.id} failed: {task.name} - {e}")

        return TaskResult(success=False, error=str(e))
```

Impl√©mentation simple pour GREEN.
  </action>
  <verify>
```bash
pytest tests/unit/services/test_task_orchestrator.py -k "test_execute_task" -v
```
Tous les tests d'ex√©cution doivent PASS.
  </verify>
  <done>
- [ ] execute_task() impl√©ment√©
- [ ] Status transitions: PENDING ‚Üí RUNNING ‚Üí COMPLETED/FAILED
- [ ] 1 commit par task (commit apr√®s status change)
- [ ] Logging ajout√©
- [ ] Tests PASS (GREEN)
  </done>
</task>

<task type="auto">
  <name>üî¥ RED: Tests retry intelligent</name>
  <files>backend/tests/unit/services/test_task_orchestrator.py</files>
  <action>
Tests pour retry intelligent (skip COMPLETED):

```python
def test_should_skip_task_returns_true_for_completed(db_session):
    """Tasks COMPLETED doivent √™tre skipp√©es."""
    orchestrator = TaskOrchestrator(db_session)

    task = MarketplaceTask(name="Test", position=1, status="COMPLETED")

    assert orchestrator.should_skip_task(task) is True

def test_should_skip_task_returns_false_for_failed(db_session):
    """Tasks FAILED doivent √™tre retry√©es."""
    orchestrator = TaskOrchestrator(db_session)

    task = MarketplaceTask(name="Test", position=1, status="FAILED")

    assert orchestrator.should_skip_task(task) is False

def test_should_skip_task_returns_false_for_pending(db_session):
    """Tasks PENDING doivent √™tre ex√©cut√©es."""
    orchestrator = TaskOrchestrator(db_session)

    task = MarketplaceTask(name="Test", position=1, status="PENDING")

    assert orchestrator.should_skip_task(task) is False

def test_execute_job_with_tasks_skips_completed(db_session, sample_job):
    """Retry doit skip les tasks COMPLETED."""
    orchestrator = TaskOrchestrator(db_session)

    # Cr√©er 3 tasks: 2 COMPLETED, 1 FAILED
    task1 = MarketplaceTask(job_id=sample_job.id, name="Task 1", position=1, status="COMPLETED")
    task2 = MarketplaceTask(job_id=sample_job.id, name="Task 2", position=2, status="COMPLETED")
    task3 = MarketplaceTask(job_id=sample_job.id, name="Task 3", position=3, status="FAILED")
    db_session.add_all([task1, task2, task3])
    db_session.commit()

    # Handlers (task1 et task2 ne doivent PAS √™tre appel√©s)
    executed = []
    def track_handler(task):
        executed.append(task.name)
        return {}

    handlers = {
        "Task 1": track_handler,
        "Task 2": track_handler,
        "Task 3": track_handler,
    }

    orchestrator.execute_job_with_tasks(sample_job, [task1, task2, task3], handlers)

    # Seulement Task 3 doit avoir √©t√© ex√©cut√©e
    assert executed == ["Task 3"]
```

Tests FAIL (m√©thodes manquantes).
  </action>
  <verify>
```bash
pytest tests/unit/services/test_task_orchestrator.py::test_should_skip_task_returns_true_for_completed -v
```
Doit FAIL (m√©thode pas impl√©ment√©e).
  </verify>
  <done>
- [ ] 4 tests retry intelligent cr√©√©s
- [ ] Tests couvrent: skip COMPLETED, retry FAILED/PENDING
- [ ] Test end-to-end avec execute_job_with_tasks
- [ ] Tests FAIL (RED)
  </done>
</task>

<task type="auto">
  <name>üü¢ GREEN: Impl√©mentation retry intelligent</name>
  <files>backend/services/marketplace/task_orchestrator.py</files>
  <action>
Ajouter m√©thodes should_skip_task() et execute_job_with_tasks():

```python
def should_skip_task(self, task: MarketplaceTask) -> bool:
    """
    V√©rifie si une task doit √™tre skipp√©e au retry.

    R√®gle: Skip seulement les tasks COMPLETED.
    """
    return task.status == "COMPLETED"

def execute_job_with_tasks(
    self,
    job: MarketplaceJob,
    tasks: List[MarketplaceTask],
    handlers: Dict[str, Callable]
) -> bool:
    """
    Ex√©cute toutes les tasks d'un job avec retry intelligent.

    Args:
        job: Le MarketplaceJob
        tasks: Liste des tasks (tri√©es par position)
        handlers: Dict {task_name: handler_function}

    Returns:
        True si toutes les tasks COMPLETED, False sinon
    """
    for task in sorted(tasks, key=lambda t: t.position):
        # Skip si d√©j√† COMPLETED (retry intelligent)
        if self.should_skip_task(task):
            logger.info(f"Task #{task.id} already completed, skipping")
            continue

        # Trouver le handler correspondant
        handler = handlers.get(task.name)
        if not handler:
            logger.warning(f"No handler found for task: {task.name}")
            continue

        # Ex√©cuter la task
        result = self.execute_task(task, handler)

        # Si √©chec, arr√™ter l'ex√©cution
        if not result.success:
            logger.error(f"Task #{task.id} failed, stopping job execution")
            return False

    # Toutes les tasks COMPLETED
    return True
```

Impl√©mentation simple pour GREEN.
  </action>
  <verify>
```bash
pytest tests/unit/services/test_task_orchestrator.py -v
```
TOUS les tests doivent PASS (GREEN complet).
  </verify>
  <done>
- [ ] should_skip_task() impl√©ment√©
- [ ] execute_job_with_tasks() impl√©ment√©
- [ ] Retry intelligent fonctionne (skip COMPLETED)
- [ ] Tous les tests PASS (GREEN complet)
  </done>
</task>

<task type="auto">
  <name>üîµ REFACTOR: Nettoyage et optimisation</name>
  <files>backend/services/marketplace/task_orchestrator.py</files>
  <action>
Refactorer pour clart√© (tests doivent rester PASS):

**Am√©liorations:**
1. Ajouter docstrings Google-style √† toutes les m√©thodes
2. Extraire dur√©e d'ex√©cution: `duration = (completed_at - started_at).total_seconds()`
3. Logger la dur√©e: `logger.info(f"Task #{task.id} completed in {duration:.2f}s")`
4. Ajouter type hints partout
5. Extraire m√©thode `_commit_with_refresh()` si r√©p√©tition

**Code propre:**
- Pas de code mort
- Noms explicites
- Pas de magic numbers

**NE PAS changer l'API publique** (tests doivent rester PASS).
  </action>
  <verify>
```bash
# Tests doivent toujours passer apr√®s refactor
pytest tests/unit/services/test_task_orchestrator.py -v

# V√©rifier coverage
pytest tests/unit/services/test_task_orchestrator.py --cov=services.marketplace.task_orchestrator --cov-report=term
```
Target: >90% coverage.
  </verify>
  <done>
- [ ] Docstrings complets (Google style)
- [ ] Dur√©e d'ex√©cution logg√©e
- [ ] Type hints partout
- [ ] Code nettoy√© (pas de duplication)
- [ ] Tests PASS apr√®s refactor
- [ ] Coverage >90%
  </done>
</task>

<task type="checkpoint:human-verify">
  <name>Test d'int√©gration avec PostgreSQL</name>
  <files>backend/tests/integration/services/test_task_orchestrator_integration.py</files>
  <action>
Cr√©er test d'int√©gration complet avec vraie DB:

```python
def test_full_retry_scenario(db_session):
    """
    Sc√©nario complet: Create job ‚Üí Create 5 tasks ‚Üí Execute ‚Üí Fail task 3 ‚Üí Retry ‚Üí Skip completed ‚Üí Success.
    """
    # 1. Cr√©er job
    job = MarketplaceJob(user_id=1, platform="vinted", action="publish", status="RUNNING")
    db_session.add(job)
    db_session.commit()

    # 2. Cr√©er 5 tasks
    orchestrator = TaskOrchestrator(db_session)
    task_names = [
        "Validate product",
        "Upload image 1/3",
        "Upload image 2/3",
        "Upload image 3/3",
        "Create listing"
    ]
    tasks = orchestrator.create_tasks(job, task_names)

    # 3. Handlers (image 2 va √©chouer la premi√®re fois)
    attempt = {"count": 0}

    def validate_handler(task): return {"valid": True}
    def upload_1_handler(task): return {"image_id": "img1"}
    def upload_2_handler(task):
        attempt["count"] += 1
        if attempt["count"] == 1:
            raise Exception("Network timeout")
        return {"image_id": "img2"}
    def upload_3_handler(task): return {"image_id": "img3"}
    def create_handler(task): return {"listing_id": "vinted_789"}

    handlers = {
        "Validate product": validate_handler,
        "Upload image 1/3": upload_1_handler,
        "Upload image 2/3": upload_2_handler,
        "Upload image 3/3": upload_3_handler,
        "Create listing": create_handler,
    }

    # 4. Premi√®re ex√©cution (va √©chouer sur task 3)
    success = orchestrator.execute_job_with_tasks(job, tasks, handlers)
    assert success is False

    # V√©rifier √©tat: task 1-2 COMPLETED, task 3 FAILED, task 4-5 PENDING
    db_session.refresh(tasks[0])
    db_session.refresh(tasks[1])
    db_session.refresh(tasks[2])
    db_session.refresh(tasks[3])
    db_session.refresh(tasks[4])

    assert tasks[0].status == "COMPLETED"
    assert tasks[1].status == "COMPLETED"
    assert tasks[2].status == "FAILED"
    assert tasks[3].status == "PENDING"
    assert tasks[4].status == "PENDING"

    # 5. Retry (doit skip task 1-2, retry task 3, continuer 4-5)
    success = orchestrator.execute_job_with_tasks(job, tasks, handlers)
    assert success is True

    # V√©rifier toutes COMPLETED
    for task in tasks:
        db_session.refresh(task)
        assert task.status == "COMPLETED"

    # V√©rifier r√©sultats stock√©s
    assert tasks[1].result == {"image_id": "img1"}
    assert tasks[2].result == {"image_id": "img2"}
    assert tasks[4].result == {"listing_id": "vinted_789"}
```

**HUMAN doit:**
1. Start PostgreSQL: `cd backend && docker-compose up -d`
2. Run migration: `alembic upgrade head`
3. Run test: `pytest tests/integration/services/test_task_orchestrator_integration.py -v`
4. V√©rifier output: "test_full_retry_scenario PASSED"
5. Confirmer dans DB: `psql -c "SELECT name, status FROM user_1.marketplace_tasks;"`
  </action>
  <verify>
HUMAN confirme:
- [ ] PostgreSQL d√©marr√©
- [ ] Migration appliqu√©e
- [ ] Test d'int√©gration PASS
- [ ] Retry intelligent fonctionne (skip completed visible en logs)
- [ ] DB montre 5 tasks toutes COMPLETED apr√®s retry
  </verify>
  <done>
HUMAN confirme test d'int√©gration r√©ussi.
  </done>
</task>

</tasks>

<verification>
**Tests unitaires:**
- [ ] Tous les tests PASS: `pytest tests/unit/services/test_task_orchestrator.py -v`
- [ ] Coverage >90%: `pytest --cov=services.marketplace.task_orchestrator --cov-report=term`
- [ ] Pas de tests flaky (run 3x, tous PASS)

**Test d'int√©gration:**
- [ ] test_full_retry_scenario PASS avec PostgreSQL r√©el
- [ ] Retry intelligent v√©rifi√© (skip COMPLETED en logs)

**Code quality:**
- [ ] Docstrings complets
- [ ] Type hints partout
- [ ] Logging informatif
- [ ] Pas de duplication
- [ ] Suit Clean Architecture

**Performance:**
- [ ] 1 commit par task confirm√© (visible en logs)
- [ ] Pas de N+1 queries
</verification>

<success_criteria>
- Cycles TDD complets (RED ‚Üí GREEN ‚Üí REFACTOR √ó 3)
- Coverage >90% pour TaskOrchestrator
- Test d'int√©gration PASS avec vraie DB
- Retry intelligent fonctionnel (skip COMPLETED)
- 1 commit par task confirm√©
- Code propre et document√©
- Pr√™t pour utilisation par les handlers (Phase 2+)
</success_criteria>

<output>
Cr√©er `.planning/phases/01-task-orchestration/01-02-SUMMARY.md` avec:

**Accomplissements:**
- TaskOrchestrator impl√©ment√© en TDD strict
- 3 cycles RED-GREEN-REFACTOR compl√©t√©s
- Test d'int√©gration valid√© avec PostgreSQL

**API cr√©√©e:**
```python
TaskOrchestrator:
  - create_tasks(job, task_names) ‚Üí List[Task]
  - execute_task(task, handler) ‚Üí TaskResult
  - should_skip_task(task) ‚Üí bool
  - execute_job_with_tasks(job, tasks, handlers) ‚Üí bool
```

**M√©triques:**
- Test coverage: XX% (target >90%)
- Tests unitaires: XX PASS
- Test d'int√©gration: 1 PASS

**Fonctionnalit√©s valid√©es:**
- ‚úÖ Cr√©ation de tasks ordonn√©es
- ‚úÖ Ex√©cution avec 1 commit/task
- ‚úÖ Retry intelligent (skip COMPLETED)
- ‚úÖ Tracking granulaire (name, position, status, result)
- ‚úÖ Logging d√©taill√©

**Prochaine √©tape:** Phase 2 - Adapter les handlers existants pour utiliser TaskOrchestrator
</output>
