---
phase: 01-task-orchestration
plan: 02
type: tdd
domain: business-logic
---

<objective>
Implement TaskOrchestrator service with granular task creation, execution, idempotent retry, and 1-commit-per-task model using Test-Driven Development.

Purpose: Core orchestration engine for marketplace task system
Output: TaskOrchestrator service with >90% test coverage, fully tested task lifecycle
</objective>

<execution_context>
./.claude/get-shit-done/workflows/execute-phase.md
./.claude/get-shit-done/references/tdd.md
./01-02-SUMMARY.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/codebase/ARCHITECTURE.md
@backend/models/user/marketplace_task.py (enhanced in Plan 01-01)
@backend/services/marketplace/marketplace_job_processor.py (existing job processor)
</context>

<feature>
**TaskOrchestrator Service** - Coordinates granular task execution with idempotent retry

**Business Rules:**
1. **Task Creation**: Given a MarketplaceJob, create ordered tasks (validate, map, upload_image_1..N, create, save)
2. **Execution**: Execute tasks sequentially, commit after EACH task (1 commit = 1 task)
3. **Skip Logic**: On retry, skip tasks with status=COMPLETED (idempotent retry)
4. **Idempotence Check**: Before executing, verify side-effects (e.g., "Did I already upload this image?")
5. **Job Status**: Job stays RUNNING until ALL tasks are COMPLETED

**Example Flow:**
```
Job "Publish Product X to Vinted":
  Task 1 (validate): PENDING ‚Üí RUNNING ‚Üí COMPLETED ‚úÖ ‚Üí commit
  Task 2 (map_data): PENDING ‚Üí RUNNING ‚Üí COMPLETED ‚úÖ ‚Üí commit
  Task 3 (upload_image_1): PENDING ‚Üí RUNNING ‚Üí FAILED ‚ùå ‚Üí commit

Retry Job:
  Task 1 (validate): COMPLETED ‚Üí Skip
  Task 2 (map_data): COMPLETED ‚Üí Skip
  Task 3 (upload_image_1): FAILED ‚Üí Check side_effects ‚Üí Retry
```

**Public API:**
```python
class TaskOrchestrator:
    def create_tasks(self, job: MarketplaceJob, task_specs: List[TaskSpec]) -> List[MarketplaceTask]
    def execute_task(self, task: MarketplaceTask) -> TaskResult
    def should_skip_task(self, task: MarketplaceTask) -> bool
    def check_idempotence(self, task: MarketplaceTask) -> bool
```
</feature>

<tasks>

<task type="auto">
  <name>üî¥ RED: Write tests for task creation</name>
  <files>backend/tests/unit/services/test_task_orchestrator.py</files>
  <action>
Create test file with pytest. Write tests FIRST (RED phase):

**Test Suite 1: Task Creation**
```python
def test_create_tasks_generates_ordered_tasks():
    """Should create tasks in correct order with step_type and step_order."""
    # Given: Job + task specs [validate, map, upload_image, create, save]
    # When: orchestrator.create_tasks(job, specs)
    # Then: 5 tasks created with step_order 1-5, status=PENDING

def test_create_tasks_sets_step_types():
    """Each task should have correct step_type from spec."""
    # Given: Specs with different step_types
    # When: create_tasks()
    # Then: Tasks have matching step_types

def test_create_tasks_marks_upload_as_non_idempotent():
    """Upload tasks should have is_idempotent=False."""
    # Given: Spec with step_type='upload_image'
    # When: create_tasks()
    # Then: Task has is_idempotent=False
```

Use pytest fixtures for db session, job, and task specs.

**DO NOT write implementation yet** - tests should FAIL initially (RED phase).
  </action>
  <verify>Run: `cd backend && pytest tests/unit/services/test_task_orchestrator.py::test_create_tasks_generates_ordered_tasks -v`
Should FAIL with "ModuleNotFoundError: No module named 'services.task_orchestrator'" (expected in RED phase)</verify>
  <done>
- [ ] Test file created with 3 tests for task creation
- [ ] Tests use pytest fixtures (db, job, specs)
- [ ] Tests FAIL as expected (no implementation yet)
- [ ] Clear assertions on step_order, step_type, is_idempotent
  </done>
</task>

<task type="auto">
  <name>üü¢ GREEN: Implement task creation (minimal)</name>
  <files>backend/services/marketplace/task_orchestrator.py</files>
  <action>
Create TaskOrchestrator service with MINIMAL implementation to make tests pass:

```python
from sqlalchemy.orm import Session
from models.user.marketplace_task import MarketplaceTask
from models.user.marketplace_job import MarketplaceJob
from typing import List, Dict, Any

class TaskSpec:
    """Specification for creating a task."""
    def __init__(self, step_type: str, is_idempotent: bool = True):
        self.step_type = step_type
        self.is_idempotent = is_idempotent

class TaskOrchestrator:
    def __init__(self, db: Session):
        self.db = db

    def create_tasks(
        self,
        job: MarketplaceJob,
        task_specs: List[TaskSpec]
    ) -> List[MarketplaceTask]:
        """Create ordered tasks for a job."""
        tasks = []
        for order, spec in enumerate(task_specs, start=1):
            task = MarketplaceTask(
                job_id=job.id,
                step_type=spec.step_type,
                step_order=order,
                is_idempotent=spec.is_idempotent,
                status='PENDING'
            )
            self.db.add(task)
            tasks.append(task)

        self.db.commit()
        return tasks
```

**DO NOT over-engineer** - just make tests pass (GREEN phase).
  </action>
  <verify>Run: `cd backend && pytest tests/unit/services/test_task_orchestrator.py -k "test_create_tasks" -v`
All task creation tests should PASS (GREEN phase)</verify>
  <done>
- [ ] TaskOrchestrator class created
- [ ] create_tasks() implemented
- [ ] TaskSpec helper class created
- [ ] All RED tests now PASS (GREEN phase)
  </done>
</task>

<task type="auto">
  <name>üî¥ RED: Write tests for task execution</name>
  <files>backend/tests/unit/services/test_task_orchestrator.py</files>
  <action>
Add tests for task execution logic (RED phase):

**Test Suite 2: Task Execution**
```python
def test_execute_task_updates_status_to_running():
    """Task status should transition PENDING ‚Üí RUNNING."""
    # Given: Task with status=PENDING
    # When: orchestrator.execute_task(task, handler_fn)
    # Then: Task status=RUNNING before execution

def test_execute_task_commits_after_completion():
    """Should commit task status after execution (1 commit per task)."""
    # Given: Task with mock handler
    # When: execute_task()
    # Then: db.commit() called, task.status=COMPLETED

def test_execute_task_stores_side_effects():
    """Handler result should be stored in task.side_effects."""
    # Given: Handler returns {"image_id": "vinted_123"}
    # When: execute_task()
    # Then: task.side_effects == {"image_id": "vinted_123"}

def test_execute_task_handles_failure():
    """Failed execution should mark task as FAILED."""
    # Given: Handler raises exception
    # When: execute_task()
    # Then: task.status=FAILED, error_message populated
```

Use pytest.raises for exception tests.
  </action>
  <verify>Run: `pytest tests/unit/services/test_task_orchestrator.py::test_execute_task_updates_status_to_running -v`
Should FAIL with AttributeError (method not implemented yet)</verify>
  <done>
- [ ] 4 new tests for task execution
- [ ] Tests cover status transitions, commits, side_effects, errors
- [ ] Tests FAIL as expected (no implementation)
  </done>
</task>

<task type="auto">
  <name>üü¢ GREEN: Implement task execution</name>
  <files>backend/services/marketplace/task_orchestrator.py</files>
  <action>
Add execute_task() method to TaskOrchestrator:

```python
from typing import Callable, Any
from datetime import datetime, timezone

class TaskResult:
    """Result of task execution."""
    def __init__(self, success: bool, side_effects: Dict[str, Any] = None, error: str = None):
        self.success = success
        self.side_effects = side_effects or {}
        self.error = error

class TaskOrchestrator:
    # ... existing code ...

    def execute_task(
        self,
        task: MarketplaceTask,
        handler: Callable[[MarketplaceTask], Dict[str, Any]]
    ) -> TaskResult:
        """
        Execute a single task with 1 commit per task.

        Args:
            task: The task to execute
            handler: Function that performs the actual work

        Returns:
            TaskResult with success status and side_effects
        """
        try:
            # Update status to RUNNING
            task.status = 'RUNNING'
            task.started_at = datetime.now(timezone.utc)
            self.db.commit()  # Commit status change

            # Execute handler
            side_effects = handler(task)

            # Mark as COMPLETED
            task.status = 'COMPLETED'
            task.completed_at = datetime.now(timezone.utc)
            task.side_effects = side_effects
            self.db.commit()  # Commit completion (1 commit per task)

            return TaskResult(success=True, side_effects=side_effects)

        except Exception as e:
            # Mark as FAILED
            task.status = 'FAILED'
            task.error_message = str(e)
            task.completed_at = datetime.now(timezone.utc)
            self.db.commit()  # Commit failure

            return TaskResult(success=False, error=str(e))
```

Add imports for datetime and timezone at top of file.
  </action>
  <verify>Run: `pytest tests/unit/services/test_task_orchestrator.py -k "test_execute_task" -v`
All execution tests should PASS</verify>
  <done>
- [ ] execute_task() implemented
- [ ] TaskResult class created
- [ ] Status transitions implemented (PENDING ‚Üí RUNNING ‚Üí COMPLETED/FAILED)
- [ ] 1 commit per task (commit after status change)
- [ ] All execution tests PASS
  </done>
</task>

<task type="auto">
  <name>üî¥ RED: Write tests for skip and idempotence</name>
  <files>backend/tests/unit/services/test_task_orchestrator.py</files>
  <action>
Add tests for skip logic and idempotence (RED phase):

**Test Suite 3: Skip & Idempotence**
```python
def test_should_skip_task_returns_true_for_completed():
    """Completed tasks should be skipped on retry."""
    # Given: Task with status=COMPLETED
    # When: orchestrator.should_skip_task(task)
    # Then: Returns True

def test_should_skip_task_returns_false_for_failed():
    """Failed tasks should NOT be skipped."""
    # Given: Task with status=FAILED
    # When: should_skip_task()
    # Then: Returns False

def test_check_idempotence_returns_true_for_idempotent_tasks():
    """Idempotent tasks don't need side-effect check."""
    # Given: Task with is_idempotent=True
    # When: check_idempotence()
    # Then: Returns True (safe to execute)

def test_check_idempotence_checks_side_effects():
    """Non-idempotent tasks check if side-effect already exists."""
    # Given: Task with is_idempotent=False, side_effects={"image_id": "123"}
    # When: check_idempotence() with verifier_fn that checks if image exists
    # Then: Returns False if image already uploaded, True otherwise
```
  </action>
  <verify>Run: `pytest tests/unit/services/test_task_orchestrator.py::test_should_skip_task_returns_true_for_completed -v`
Should FAIL (methods not implemented)</verify>
  <done>
- [ ] 4 tests for skip and idempotence logic
- [ ] Tests cover COMPLETED skip, FAILED retry, idempotent bypass, side-effect check
- [ ] Tests FAIL as expected
  </done>
</task>

<task type="auto">
  <name>üü¢ GREEN: Implement skip and idempotence</name>
  <files>backend/services/marketplace/task_orchestrator.py</files>
  <action>
Add skip and idempotence methods:

```python
class TaskOrchestrator:
    # ... existing code ...

    def should_skip_task(self, task: MarketplaceTask) -> bool:
        """
        Check if task should be skipped on retry.

        Skip logic: Only skip COMPLETED tasks.
        Retry FAILED and PENDING tasks.
        """
        return task.status == 'COMPLETED'

    def check_idempotence(
        self,
        task: MarketplaceTask,
        verifier: Callable[[Dict[str, Any]], bool] = None
    ) -> bool:
        """
        Check if task can be executed safely (idempotence check).

        Args:
            task: Task to check
            verifier: Optional function to verify side-effects still exist

        Returns:
            True if safe to execute, False if side-effect already exists
        """
        # Idempotent tasks are always safe to retry
        if task.is_idempotent:
            return True

        # Non-idempotent tasks: check side-effects
        if task.side_effects and verifier:
            # If side-effect exists, DON'T retry (already done)
            return not verifier(task.side_effects)

        # No side-effects recorded yet, safe to execute
        return True
```

Add type hints for Callable at top of file.
  </action>
  <verify>Run: `pytest tests/unit/services/test_task_orchestrator.py -v`
ALL tests should PASS (full GREEN phase)</verify>
  <done>
- [ ] should_skip_task() implemented
- [ ] check_idempotence() implemented
- [ ] All skip and idempotence tests PASS
- [ ] Full test suite passes (>90% coverage expected)
  </done>
</task>

<task type="auto">
  <name>üîµ REFACTOR: Code cleanup and optimization</name>
  <files>backend/services/marketplace/task_orchestrator.py</files>
  <action>
Refactor for clarity and maintainability (tests should still pass):

**Improvements:**
1. Add docstrings to all methods (Google style)
2. Extract status transition logic to private method: `_transition_status(task, new_status)`
3. Add type hints to all parameters and returns
4. Add logging for task lifecycle events:
   - `logger.info(f"Task #{task.id} started: {task.step_type}")`
   - `logger.info(f"Task #{task.id} completed in {duration}s")`
   - `logger.error(f"Task #{task.id} failed: {error}")`
5. Extract commit logic to `_commit_task_state(task)` for consistency

**DO NOT change public API** - only internal refactoring.
**Keep tests passing** - run after each change.

Add logger import: `from shared.logging_setup import get_logger`
  </action>
  <verify>Run: `pytest tests/unit/services/test_task_orchestrator.py -v && pytest tests/unit/services/test_task_orchestrator.py --cov=services.marketplace.task_orchestrator --cov-report=term`
Should show >90% coverage, all tests PASS</verify>
  <done>
- [ ] Docstrings added to all methods
- [ ] Logging added for task lifecycle
- [ ] Private methods extracted for clarity
- [ ] Tests still pass after refactoring
- [ ] Coverage >90%
  </done>
</task>

<task type="checkpoint:human-verify">
  <name>Integration test: Full task lifecycle</name>
  <files>backend/tests/integration/services/test_task_orchestrator_integration.py</files>
  <action>
Create integration test that verifies full lifecycle with real database:

```python
def test_full_task_lifecycle_with_retry(db_session):
    """
    Integration test: Create job ‚Üí Create tasks ‚Üí Execute ‚Üí Fail one ‚Üí Retry ‚Üí Skip completed ‚Üí Complete.
    """
    # Setup: Create job with 3 tasks
    orchestrator = TaskOrchestrator(db_session)
    job = create_test_job()
    specs = [
        TaskSpec('validate', is_idempotent=True),
        TaskSpec('upload_image', is_idempotent=False),
        TaskSpec('save', is_idempotent=True),
    ]
    tasks = orchestrator.create_tasks(job, specs)

    # Execute tasks (simulate failure on task 2)
    def validate_handler(task): return {"validated": True}
    def upload_handler(task): raise Exception("Network error")
    def save_handler(task): return {"saved": True}

    orchestrator.execute_task(tasks[0], validate_handler)  # ‚úÖ COMPLETED
    orchestrator.execute_task(tasks[1], upload_handler)    # ‚ùå FAILED
    # Task 3 not executed (job failed)

    # Verify: Task 1 completed, Task 2 failed, Task 3 pending
    db_session.refresh(tasks[0])
    assert tasks[0].status == 'COMPLETED'
    assert tasks[1].status == 'FAILED'
    assert tasks[2].status == 'PENDING'

    # Retry: Should skip task 1, retry task 2, execute task 3
    for task in tasks:
        db_session.refresh(task)
        if orchestrator.should_skip_task(task):
            continue
        if task.step_type == 'upload_image':
            # Simulate fixed upload
            orchestrator.execute_task(task, lambda t: {"image_id": "vinted_123"})
        else:
            orchestrator.execute_task(task, save_handler)

    # Verify: All tasks completed
    for task in tasks:
        db_session.refresh(task)
        assert task.status == 'COMPLETED'
```

Run this test to verify TaskOrchestrator works end-to-end with PostgreSQL.
  </action>
  <verify>
HUMAN: Run integration test with real database:
1. Start PostgreSQL: `cd backend && docker-compose up -d`
2. Run test: `pytest tests/integration/services/test_task_orchestrator_integration.py -v`
3. Verify output shows: "3 tasks created ‚Üí 1 completed, 1 failed, 1 pending ‚Üí retry skips completed ‚Üí all completed"
4. Confirm in database: `psql -c "SELECT id, step_type, status FROM user_1.marketplace_tasks ORDER BY step_order;"`
  </verify>
  <done>
HUMAN confirms:
- [ ] Integration test created
- [ ] Test passes with real PostgreSQL
- [ ] Full lifecycle works: create ‚Üí execute ‚Üí fail ‚Üí retry ‚Üí skip completed ‚Üí complete
- [ ] Database shows correct task states
  </done>
</task>

</tasks>

<verification>
**Unit Tests (Automated):**
- [ ] All unit tests pass: `pytest tests/unit/services/test_task_orchestrator.py -v`
- [ ] Coverage >90%: `pytest --cov=services.marketplace.task_orchestrator --cov-report=term`
- [ ] No flaky tests (run 3 times, all pass)

**Integration Test (Human Verify):**
- [ ] Integration test passes with real database
- [ ] Full retry scenario works (skip completed, retry failed)

**Code Quality:**
- [ ] All methods have docstrings
- [ ] Logging added for task lifecycle
- [ ] Type hints on all public methods
- [ ] No code duplication

**Architecture:**
- [ ] Follows Clean Architecture (service layer)
- [ ] Uses existing SQLAlchemy models
- [ ] No direct database access (uses ORM)
- [ ] Idempotent by design
</verification>

<success_criteria>
- All TDD cycles complete (RED ‚Üí GREEN ‚Üí REFACTOR)
- Test coverage >90% for TaskOrchestrator
- Integration test passes with real database
- Full task lifecycle verified (create ‚Üí execute ‚Üí retry ‚Üí skip completed)
- 1 commit per task confirmed (visible in DB during execution)
- Code is clean, documented, and maintainable
</success_criteria>

<output>
Create `.planning/phases/01-task-orchestration/01-02-SUMMARY.md` with:
- TDD cycle results (RED ‚Üí GREEN ‚Üí REFACTOR for each feature)
- Test coverage report (% and line numbers)
- Integration test results
- TaskOrchestrator API documentation
- Any issues encountered
- Next steps (ready for Phase 2: Base Handler Unification)
</output>
